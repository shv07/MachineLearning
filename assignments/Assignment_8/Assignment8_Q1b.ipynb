{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Assignment 8, Q1 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd\n",
    "import pandas as pd\n",
    "from autograd import elementwise_grad as egrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(self,data):\n",
    "    data = np.array(data)\n",
    "    return np.maximum(data, 0)   #element_wise max\n",
    "def sigmoid(self,data):\n",
    "    data = np.array(data)\n",
    "    return 1/(1+np.exp(-1*data))\n",
    "def softmax(self,data):\n",
    "    data_ = []\n",
    "    for i in np.array(data):\n",
    "        tmp_ = sum([np.exp(k) for k in i])\n",
    "        tmp = np.exp(i)/tmp_\n",
    "        data_.append(tmp)\n",
    "    return np.array(data_)\n",
    "def linear(self, data):\n",
    "    return data\n",
    "\n",
    "\n",
    "def MSE(weights, y):\n",
    "    return sum((y-y_)**2)/len(y)\n",
    "     \n",
    "        \n",
    "class NN:\n",
    "    def __init__(self, data, n_layers, hidden_unit_list, activn_fn_list, cost_fn, lr = 0.01):\n",
    "        '''\n",
    "        data - n_samples x d_features\n",
    "        total no. of layers - n_layers + output_layer\n",
    "        hidden_unit_list - no. of nodes in each layer\n",
    "        activation fn - string_names of the activn fns for each of n+1 layers\n",
    "        '''\n",
    "        \n",
    "        self.data = data\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_unit_list = hidden_unit_list\n",
    "        self.activn_fn_list = activn_fn_list\n",
    "        self.cost_fn = cost_fn\n",
    "        self.weights = {}   # shape {layer_i : [weight_matrix]}  n+1 keys\n",
    "        self.bias = {}      #shape {layer_i: bias_vector}\n",
    "        self.learnin_rate = lr\n",
    "\n",
    "        #initialise the weights and bias\n",
    "        for i in range(self.n_layers+1):\n",
    "            bias = np.random.randn(self.hidden_unit_list[i])    #bias_ith_layer  = n_hidden_nodes_of_ith_layer x 1\n",
    "            self.bias[i] = bias\n",
    "            if i == 0:\n",
    "                weight = np.random.randn(len(self.data[0]), self.hidden_unit_list[i])\n",
    "                self.weights[i] = weight\n",
    "            else:\n",
    "                weight = np.random.randn(self.hidden_unit_list[i-1], self.hidden_unit_list[i])\n",
    "                self.weights[i] = weight\n",
    "        \n",
    "        \n",
    "    def forward(self, weights):        \n",
    "        forward = self.data \n",
    "        for i in range(self.n_layers+1):\n",
    "            forward = forward @ weights[i] + self.bias[i]\n",
    "            forward = self.activn_fn_list[i](forward)\n",
    "        return forward\n",
    "    \n",
    "    def backward(self, labels):\n",
    "        \n",
    "        \n",
    "        for i in range(self.n_layers+1, -1,-1):\n",
    "            d_i = egrad(mse)\n",
    "        targets = labels\n",
    "        i = self.n_layers\n",
    "        y = self.activn_fn_list[i]\n",
    "        deltab = np.multiply(y, np.multiply(1-y, targets-y))\n",
    "        deltaw = np.matmul(np.asarray(self.activn_fn_list[i-1]).T, deltab)\n",
    "        new_weights = self.weights[i-1] - self.learning_rate * deltaw\n",
    "        new_bias = self.layers[i-1].bias_for_layer - self.learning_rate * deltab\n",
    "        for i in range(i-1, 0, -1):\n",
    "            y = self.layers[i].activations\n",
    "            deltab = np.multiply(y, np.multiply(1-y, np.sum(np.multiply(new_bias, self.layers[i].bias_for_layer)).T))\n",
    "            deltaw = np.matmul(np.asarray(self.layers[i-1].activations).T, np.multiply(y, np.multiply(1-y, np.sum(np.multiply(new_weights, self.layers[i].weights_for_layer),axis=1).T)))\n",
    "            self.layers[i].weights_for_layer = new_weights\n",
    "            self.layers[i].bias_for_layer = new_bias\n",
    "            new_weights = self.layers[i-1].weights_for_layer - self.learning_rate * deltaw\n",
    "            new_bias = self.layers[i-1].bias_for_layer - self.learning_rate * deltab\n",
    "        self.layers[0].weights_for_layer = new_weights\n",
    "        self.layers[0].bias_for_layer = new_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(5,-1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
